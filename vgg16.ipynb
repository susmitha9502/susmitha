{"cells":[{"cell_type":"code","execution_count":1,"id":"DgaWoAgJLI8J","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29499,"status":"ok","timestamp":1728793400645,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"DgaWoAgJLI8J","outputId":"cb1d25dc-28b4-4b3e-8d58-b8dbf7516cb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"16e16f34","metadata":{"id":"16e16f34"},"outputs":[],"source":["# Goto https://keras.io/api/applications/ for getting any transfer learning model based on ImageNet\n","\"\"\"ImageNet is a large-scale image database that was created to serve as a benchmark for visual object recognition research.\n","It contains over 14 million labeled images spanning 22,000 categories.The ImageNet project gained significant attention in the\n","computer vision community when it launched the Large Scale Visual Recognition Challenge (ILSVRC) in 2010, which was a\n","competition designed to evaluate the accuracy of algorithms for object detection and image classification tasks. One of the\n","key outcomes of the ILSVRC competition was the development of deep convolutional neural networks (CNNs), such as AlexNet, VGG,\n","and ResNet, which dramatically improved the state-of-the-art performance on object recognition tasks.\"\"\""]},{"cell_type":"code","execution_count":null,"id":"2939144e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"elapsed":15642,"status":"error","timestamp":1724304929486,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"2939144e","outputId":"ec93f564-92ea-46c3-e6db-f6ce0fbb9439"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (/usr/local/lib/python3.10/dist-packages/keras/api/preprocessing/image/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3cf2c9c0a178>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'ImageDataGenerator' from 'keras.preprocessing.image' (/usr/local/lib/python3.10/dist-packages/keras/api/preprocessing/image/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from keras.layers import Input, Lambda, Dense, Flatten\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","#from keras.applications.resnet import ResNet50\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","import numpy as np\n","from glob import glob\n","import matplotlib.pyplot as plt\n","\n","# re-size all the images to this\n","IMAGE_SIZE = [224, 224]\n","\n","train_path = '/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train'\n","valid_path = '/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/test'\n","\n","# add preprocessing layer to the front of VGG(include=False means donot add the last layer)\n","vgg = VGG16(input_shape=[224,224,3], weights='imagenet', include_top=False)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"YS2jcM-bfW7O","metadata":{"id":"YS2jcM-bfW7O"},"outputs":[],"source":["# execute this only once... if you run again, skip this part\n","import zipfile\n","with zipfile.ZipFile(\"/content/drive/MyDrive/Artificial_Intelligence_Engineering/CV/5_train.zip\", 'r') as zip_ref:        # get the path for train.zip using copypath. paste the link within the quotation\n","     # creates new folder 'cats_dogs_images' and saves all images present in train.zip\n","    zip_ref.extractall(\"/content/extracted_images\")  # create a new folder, rename it and get the path of it using copypath.. paste the link here\n","\n"]},{"cell_type":"code","execution_count":null,"id":"1fd03ec2","metadata":{"id":"1fd03ec2"},"outputs":[],"source":["import os, shutil, pathlib\n","\n","original_dir = pathlib.Path(\"/content/extracted_images/train\") # get the path of train folder in cats_dogs_images using copypath and paste the link here\n","new_base_dir = pathlib.Path(r\"/content/extracted_images/dataset\") # create a new folder, rename it and get the path using copypath and paste the link here.\n","\n","def make_subset(subset_name, start_index, end_index):\n","    for category in (\"cat\", \"dog\"):\n","        dir = new_base_dir / subset_name / category\n","        os.makedirs(dir)\n","        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n","        for fname in fnames:\n","            shutil.copyfile(src=original_dir / fname,\n","                            dst=dir / fname)\n","\n","# calling the function thrice to create 3 subsets.\n","make_subset(\"train\", start_index=0, end_index=1000)\n","make_subset(\"validation\", start_index=1000, end_index=1500)\n","make_subset(\"test\", start_index=1500, end_index=2500)"]},{"cell_type":"code","execution_count":null,"id":"7a2eb6fc","metadata":{"id":"7a2eb6fc"},"outputs":[],"source":["import os\n","import cv2\n","labels = ['cat', 'dog']\n","img_size = 224\n","def get_data(data_dir):\n","    data = []\n","    for label in labels:\n","        path = os.path.join(data_dir, label)\n","        class_num = labels.index(label)\n","        for img in os.listdir(path):\n","            try:\n","                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n","                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n","                data.append([resized_arr, class_num])\n","            except Exception as e:\n","                print(e)\n","    return np.array(data,dtype='object')"]},{"cell_type":"code","execution_count":null,"id":"5ff52c9b","metadata":{"id":"5ff52c9b"},"outputs":[],"source":["train = get_data(r'/content/extracted_images/dataset/train')\n","test = get_data(r'/content/extracted_images/dataset/test')"]},{"cell_type":"code","execution_count":null,"id":"pxYltvgKEhpx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":425,"status":"ok","timestamp":1720102163993,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"pxYltvgKEhpx","outputId":"4e093d14-8a8b-4548-f147-47cee1529e25"},"outputs":[{"name":"stdout","output_type":"stream","text":["2000\n","2000\n"]}],"source":["print(len(train))\n","print(len(test))"]},{"cell_type":"code","execution_count":null,"id":"f5Nqc9UFbuj3","metadata":{"id":"f5Nqc9UFbuj3"},"outputs":[],"source":["x_train = []\n","y_train = []\n","x_val = []\n","y_val = []\n","\n","for feature, label in train:\n","  x_train.append(feature)\n","  y_train.append(label)\n","\n","for feature, label in test:\n","  x_val.append(feature)\n","  y_val.append(label)\n","\n","# Normalize the data\n","x_train = np.array(x_train).astype('float32') / 255\n","x_val = np.array(x_val).astype('float32') / 255"]},{"cell_type":"code","execution_count":null,"id":"0m91oEMRKwgR","metadata":{"id":"0m91oEMRKwgR"},"outputs":[],"source":["len(train)"]},{"cell_type":"code","execution_count":null,"id":"7kUbu_2vXbzr","metadata":{"id":"7kUbu_2vXbzr"},"outputs":[],"source":["len(test)"]},{"cell_type":"code","execution_count":null,"id":"IF-Q4MkGXm0A","metadata":{"id":"IF-Q4MkGXm0A"},"outputs":[],"source":["len(train[0])"]},{"cell_type":"code","execution_count":null,"id":"TQ79VOsfXnSs","metadata":{"id":"TQ79VOsfXnSs"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"4b34edba","metadata":{"id":"4b34edba"},"outputs":[],"source":["# don't train existing weights\n","for layer in vgg.layers:\n","    layer.trainable = False"]},{"cell_type":"code","execution_count":null,"id":"4SRjxwrNGJHY","metadata":{"id":"4SRjxwrNGJHY"},"outputs":[],"source":["folders = glob('/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train')"]},{"cell_type":"code","execution_count":null,"id":"aktfgP18GLBd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1720102482420,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"aktfgP18GLBd","outputId":"aff85cd3-62ad-4f2c-a0a5-acfe269626fd"},"outputs":[{"data":{"text/plain":["list"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["type(folders)"]},{"cell_type":"code","execution_count":null,"id":"SapMiN3vGPnq","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":520,"status":"ok","timestamp":1720102490167,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"SapMiN3vGPnq","outputId":"1a70733e-fd58-4e5c-a450-ef3675659d79"},"outputs":[{"data":{"text/plain":["['/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":null,"id":"b61f63c5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":510,"status":"ok","timestamp":1720102780520,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"b61f63c5","outputId":"c88534bd-7923-4012-d8a7-f291f8b87af3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," dense (Dense)               (None, 1)                 25089     \n","                                                                 \n","=================================================================\n","Total params: 14739777 (56.23 MB)\n","Trainable params: 25089 (98.00 KB)\n","Non-trainable params: 14714688 (56.13 MB)\n","_________________________________________________________________\n"]}],"source":["folders = glob('/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train')\n","\n","\n","# our layers - you can add more if you want\n","x = Flatten()(vgg.output)\n","# x = Dense(1000, activation='relu')(x)\n","prediction = Dense(1, activation='sigmoid')(x)\n","\n","# create a model object\n","model = Model(inputs=vgg.input, outputs=prediction)\n","\n","# view the structure of the model\n","model.summary()\n","\n"]},{"cell_type":"code","execution_count":null,"id":"2a4668be","metadata":{"id":"2a4668be"},"outputs":[],"source":["# tell the model what cost and optimization method to use\n","model.compile(\n","  loss='binary_crossentropy',\n","  optimizer='adam',\n","  metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"id":"fu6P_dioIYra","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":406,"status":"ok","timestamp":1720103059136,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"fu6P_dioIYra","outputId":"487750db-a66c-4630-d0a2-e7bf564bf322"},"outputs":[{"data":{"text/plain":["['/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train/cat',\n"," '/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train/dog']"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["glob('/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train/*')"]},{"cell_type":"code","execution_count":null,"id":"U18O4wL0b6sm","metadata":{"id":"U18O4wL0b6sm"},"outputs":[],"source":["folders = glob('/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train/*')\n","print(type(folders))\n","print(folders[0])\n","print(len(folders))"]},{"cell_type":"code","execution_count":null,"id":"HErb_amKj9wa","metadata":{"id":"HErb_amKj9wa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"99eb89b9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7354,"status":"ok","timestamp":1720103496436,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"99eb89b9","outputId":"8e36d73b-352a-40df-9379-8784396f1aa2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1073 images belonging to 2 classes.\n","Found 1133 images belonging to 2 classes.\n"]}],"source":["\n","# useful for getting number of classes\n","folders = glob('/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train/*')\n","\n","\n","\n","\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/train',\n","                                                 target_size = (224, 224),\n","                                                 batch_size = 32,\n","                                                 class_mode = 'binary')\n","\n","test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/CNN/cats_dogs_images/image_seggrated/test',\n","                                            target_size = (224, 224),\n","                                            batch_size = 32,\n","                                            class_mode = 'binary')\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ltZ8wi73KbMS","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1720103595753,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"ltZ8wi73KbMS","outputId":"8cc1af2f-7e49-4acb-fc96-e56c0cc9c7ea"},"outputs":[{"data":{"text/plain":["33.53125"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["1073/32"]},{"cell_type":"code","execution_count":null,"id":"dh-y0gXQKWV-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1720103570636,"user":{"displayName":"Nisarg Nikhil","userId":"14369246193582587886"},"user_tz":-330},"id":"dh-y0gXQKWV-","outputId":"11e7f861-998f-4cf7-f3e3-6e1c4b3911db"},"outputs":[{"data":{"text/plain":["34"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["len(training_set)"]},{"cell_type":"code","execution_count":null,"id":"xBV-K2wlguVS","metadata":{"id":"xBV-K2wlguVS"},"outputs":[],"source":["type(training_set)"]},{"cell_type":"code","execution_count":null,"id":"rLfPr2_mg9mf","metadata":{"id":"rLfPr2_mg9mf"},"outputs":[],"source":["len(training_set)"]},{"cell_type":"code","execution_count":null,"id":"9525cf1e","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"9525cf1e","outputId":"60cf2fef-9026-4867-d148-7e8b01bae17b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","34/34 [==============================] - 1347s 40s/step - loss: 0.8036 - accuracy: 0.5000 - val_loss: 0.7231 - val_accuracy: 0.5000\n","Epoch 2/5\n","34/34 [==============================] - 1271s 38s/step - loss: 0.7081 - accuracy: 0.5000 - val_loss: 0.7061 - val_accuracy: 0.5000\n","Epoch 3/5\n","34/34 [==============================] - ETA: 0s - loss: 0.7012 - accuracy: 0.5000 "]}],"source":["# fit the model\n","r = model.fit(\n","  training_set,\n","  validation_data=test_set,\n","  epochs=5,\n","  steps_per_epoch=len(training_set),\n","  validation_steps=len(test_set)\n",")"]},{"cell_type":"code","execution_count":null,"id":"eaff2be8","metadata":{"id":"eaff2be8"},"outputs":[],"source":["'''r=model.fit_generator(training_set,\n","                         samples_per_epoch = 8000,\n","                         nb_epoch = 5,\n","                         validation_data = test_set,\n","                         nb_val_samples = 2000)'''\n","\n","\n","# loss\n","plt.plot(r.history['loss'], label='train loss')\n","plt.plot(r.history['val_loss'], label='val loss')\n","plt.legend()\n","plt.show()\n","plt.savefig('LossVal_loss')\n","\n","# accuracies\n","plt.plot(r.history['acc'], label='train acc')\n","plt.plot(r.history['val_acc'], label='val acc')\n","plt.legend()\n","plt.show()\n","plt.savefig('AccVal_acc')\n","\n","import tensorflow as tf\n","\n","from keras.models import load_model\n","\n","model.save('facefeatures_new_model.h5')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":5}